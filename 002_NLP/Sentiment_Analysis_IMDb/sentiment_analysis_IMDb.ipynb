{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af81235",
   "metadata": {},
   "source": [
    "# Title - Sentiment Analysis IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c4b1e",
   "metadata": {},
   "source": [
    "### Core Machine Learning\n",
    "*   **`tensorflow as tf`**: This is the fundamental library for the entire project. TensorFlow provides the tools to build, train, and deploy machine learning models, especially neural networks.\n",
    "*   **`tensorflow.keras.layers`**: Keras is TensorFlow's user-friendly API for building neural networks. The `layers` module provides the building blocks for your model, such as `Embedding` (to convert words into numerical vectors), `Dense` (standard neural network layers), and `Dropout` (to prevent overfitting).\n",
    "*   **`tensorflow.keras.losses`**: This provides the loss functions, which are used to measure how inaccurate the model's predictions are. The goal of training is to minimize this value. The tutorial uses `BinaryCrossentropy` for a two-class problem (positive/negative).\n",
    "\n",
    "### Data Handling and Preparation\n",
    "*   **`numpy as np`**: A core library for numerical operations. TensorFlow is tightly integrated with NumPy, and it's often used to handle the numerical data (vectors and matrices) that the model processes.\n",
    "*   **`pandas as pd`**: While not used in the first cell of your notebook, Pandas is essential for reading, writing, and manipulating structured data, such as from a CSV file. You would typically use it to load your dataset into a structure called a DataFrame.\n",
    "*   **`os`**: This module helps interact with the operating system, primarily for handling file paths and directories. The tutorial uses it to manage the dataset files after they are downloaded.\n",
    "*   **`shutil`**: Provides high-level file operations. The tutorial uses it to remove the downloaded dataset directory to clean up the workspace.\n",
    "\n",
    "### Text Processing\n",
    "*   **`re`**: The regular expression module. It's crucial for cleaning text data. In the tutorial, it's used to remove HTML tags like `<br />` from the movie reviews.\n",
    "*   **`string`**: This module contains common string constants. The tutorial uses `string.punctuation` to easily access a list of all punctuation marks that should be stripped from the text.\n",
    "\n",
    "### Data Visualization\n",
    "*   **`matplotlib.pyplot as plt`**: The primary library for creating plots and graphs. It's used at the end of the tutorial to visualize the model's training and validation accuracy and loss over time. This helps you understand if the model is learning effectively or overfitting.\n",
    "*   **`seaborn as sns`**: Built on top of Matplotlib, Seaborn provides more advanced and aesthetically pleasing statistical plots. While not strictly required by the tutorial, it's often used for tasks like creating confusion matrices or visualizing data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd5e918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdcba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and explore the IMDB dataset\n",
    "\n",
    "# 1. Get the data from the web\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e4afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703ce880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aclImdb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311aa374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'aclImdb', 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'aclImdb', 'test')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f32443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
     ]
    }
   ],
   "source": [
    "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
    "with open(sample_file) as f:\n",
    "  print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "548ae79a",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: '.\\\\aclImdb_v1\\\\aclImdb\\\\train\\\\unsup\\\\14837_0.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m remove_dir = os.path.join(train_dir, \u001b[33m'\u001b[39m\u001b[33munsup\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\shutil.py:790\u001b[39m, in \u001b[36mrmtree\u001b[39m\u001b[34m(path, ignore_errors, onerror, onexc, dir_fd)\u001b[39m\n\u001b[32m    788\u001b[39m     \u001b[38;5;66;03m# can't continue even if onexc hook returns\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monexc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\shutil.py:629\u001b[39m, in \u001b[36m_rmtree_unsafe\u001b[39m\u001b[34m(path, onexc)\u001b[39m\n\u001b[32m    627\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    628\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m             \u001b[43monexc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    631\u001b[39m     os.rmdir(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\shutil.py:625\u001b[39m, in \u001b[36m_rmtree_unsafe\u001b[39m\u001b[34m(path, onexc)\u001b[39m\n\u001b[32m    623\u001b[39m fullname = os.path.join(dirpath, name)\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mPermissionError\u001b[39m: [WinError 32] The process cannot access the file because it is being used by another process: '.\\\\aclImdb_v1\\\\aclImdb\\\\train\\\\unsup\\\\14837_0.txt'"
     ]
    }
   ],
   "source": [
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(\"Review\", text_batch.numpy()[i])\n",
    "    print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f299051",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee93263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b186fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da44ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text-only dataset (without labels), then call adapt\n",
    "\n",
    "print(\"Starting to adapt the vectorize_layer...\")\n",
    "\n",
    "vectorize_layer.adapt(raw_train_ds.map(lambda text, label: text))\n",
    "\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "print(\"Adaptation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39435a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", raw_train_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
    "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f52d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "max_index = len(vocab) - 1\n",
    "print(f\"Total vocabulary size: {len(vocab)}\")\n",
    "print(f\"Maximum index: {max_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d63ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the first 20 words in the vocabulary\n",
    "print(f\"First 20 words in the vocabulary:\\n {vocab[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096e962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the last 20 words in the vocabulary\n",
    "print(f\"Last 20 words in the vocabulary:\\n {vocab[-20:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57bbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ee05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae230cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features, embedding_dim),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17778bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda2529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a602bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fbb0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37199c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "metrics = export_model.evaluate(raw_test_ds, return_dict=True)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = tf.constant([\n",
    "  \"The movie was great!\",\n",
    "  \"The movie was okay.\",\n",
    "  \"The movie was terrible...\"\n",
    "])\n",
    "\n",
    "export_model.predict(examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
