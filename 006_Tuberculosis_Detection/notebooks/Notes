The next logical step is to build the model architecture.

  We are going to create a simple but effective Convolutional Neural Network (CNN) from scratch. As we discussed, a CNN is the standard choice for
  image classification. Our model will be a Sequential stack of layers. Here's the plan for the architecture:

   1. `Conv2D` Layer: The first convolutional layer will look for simple, low-level features like edges and curves. It will have 32 filters. This layer
      needs to know the input_shape of our images (150x150 pixels, 3 color channels).
   2. `MaxPooling2D` Layer: This will downsample the feature map, making the model more efficient and helping it generalize.
   3. More `Conv2D` and `MaxPooling2D` Layers: We will stack a few more of these pairs. Each subsequent Conv2D layer will have more filters (64, then
      128) and will learn to recognize more complex patterns by combining the features from the previous layer.
   4. `Flatten` Layer: This layer will take the final 2D feature map from the convolutional section and unroll it into a single, long 1D vector. This
      prepares the data to be fed into the final, fully-connected layers.
   5. `Dense` Layer: A standard, fully-connected layer with 512 neurons that will perform high-level reasoning on the features extracted by the
      convolutional layers.
   6. `Dense` (Output) Layer: The final layer. It will have only one neuron. This neuron will use a sigmoid activation function, which squashes its
      output to be a number between 0 and 1. We can interpret this number as the probability that the image belongs to the "tuberculosis" class.

 ---------
  In terms of model architecture, are there other models we can use aside from CNN? i heard about pre-trained models like ResNet or MobileNet.  
  
  You're bringing up the most powerful and widely-used technique in modern computer vision: Transfer Learning with pre-trained
  models.

  You are absolutely correct. While building a CNN from scratch is a great way to learn the fundamentals, for most real-world applications,
  especially with a small dataset like ours, using a pre-trained model like ResNet or MobileNet is the superior approach.

  What is Transfer Learning?

  Think of it like this: instead of teaching a baby to recognize a specific type of cancer in an X-ray (by first teaching it what lines, shapes, and
  textures are), you take an experienced radiologist and just teach them the specific patterns of this new disease. The radiologist already has a
  vast, pre-existing knowledge of visual patterns in medical images.

  A pre-trained model is that experienced radiologist.
   1. It's Pre-Trained: Models like ResNet50 or MobileNetV2 have already been trained on a massive dataset called ImageNet (over a million images of
      1,000 different objects).
   2. It Has "Learned" Vision: Because of this training, the model is already an expert at recognizing a huge library of visual features—edges,
      textures, shapes, patterns, etc. This "knowledge" is stored in its trained weights.
   3. We Adapt It: We can leverage this powerful, built-in knowledge and adapt it to our specific task of identifying tuberculosis.

  How We Adapt It: Fine-Tuning

  The most common strategy, and the one we would use, is called fine-tuning:
   1. Load the Pre-Trained Model: We load the model (e.g., MobileNetV2) with its ImageNet weights, but we chop off its original top classification layer
      (the part that predicts 1,000 classes).
   2. Freeze the Base: We "freeze" the weights of most of the model's layers. This prevents the pre-existing knowledge from being destroyed during
      training.
   3. Add Our Own Classifier: We stack our own small classifier on top—usually a couple of Dense layers, just like in our current model.
   4. Train: We then train this new, combined model on our X-ray data. We are only training the new layers we added and, optionally, "fine-tuning"
      (making small adjustments to) the last few layers of the pre-trained base.

  ResNet vs. MobileNet

  The models you mentioned are two of the most popular choices:
   * ResNet (e.g., ResNet50): Extremely powerful and known for achieving state-of-the-art accuracy. It's a fantastic choice when performance is the top
     priority.
   * MobileNet (e.g., MobileNetV2): Designed to be very lightweight and fast, making it ideal for mobile devices or applications where computational
     resources are limited. It offers a great trade-off between speed and accuracy.

  For our project, using a pre-trained model is the logical next step and will almost certainly give us a much better result.

  Thank you for asking this. It shows you're thinking ahead. For now, let's finish building our simple model to get a baseline understanding. Once we
  see how it performs, we can truly appreciate the power of transfer learning when we try it next.


  Model - Sequential table.  
  Of course. My apologies for not explaining it thoroughly. The model.summary() table is a crucial tool, and understanding it is fundamental. Let's
  walk through it as if you're seeing it for the first time.

  Think of the summary table as the architect's blueprint for your model. It shows every component, how data flows through it, and how complex each
  part is.

  The table has three columns: Layer (type), Output Shape, and Param #.

  ---

  1. Layer (type)

  This column simply lists the name and type of each layer in your model, in the exact order you defined them.

   * conv2d (Conv2D): This is a 2D convolutional layer. conv2d is its unique, auto-generated name, and (Conv2D) is its type. You'll see subsequent
     layers named conv2d_1, conv2d_2, etc.
   * max_pooling2d (MaxPooling2D): A max pooling layer.
   * flatten (Flatten): The layer that unrolls the data into a single line.
   * dense (Dense): A standard, fully-connected layer.

  This column is your first sanity check to ensure you've built the model in the sequence you intended.

  ---

  2. Output Shape

  This is the most important column for understanding the flow of data. It shows the shape of your data after it passes through that specific layer.
  The shape is a tuple, like (None, 148, 148, 32).

   * The `None` Dimension: The first value is always None. This represents the batch size. Since your model can accept a batch of 10 images, 20 images,
     or any number, this dimension is left flexible (None). You can generally ignore it when trying to understand the transformation of a single image.

   * The Data Dimensions: The rest of the numbers describe the shape of a single piece of data.
       * For `Conv2D` or `MaxPooling2D` layers, the shape is (height, width, channels).
           * Let's trace the first two layers. Your input is (150, 150, 3).
           * The first conv2d layer's output is (None, 148, 148, 32). This means the 150x150 image was transformed into a 148x148 "feature map," and
             the number of channels grew from 3 (RGB) to 32 (one for each filter we used).
           * The next max_pooling2d layer's output is (None, 74, 74, 32). The number of channels (32) is unchanged, but the height and width were cut
             in half (148 -> 74). This is the "downsampling" we talked about.
       * For a `Flatten` layer, the output shape will be (None, N). It takes the final multi-dimensional feature map and unrolls it into a single long
         vector. For example, a (7, 7, 128) feature map becomes a vector of length 6,272 (since 7  7  128 = 6272).
       * For a `Dense` layer, the output is (None, number_of_neurons). Our final output layer has a shape of (None, 1), representing the single neuron
         that gives us our final probability.

  By following this column from top to bottom, you can trace the exact journey of your data as it's filtered, shrunk, and processed. It's the best
  way to debug your model's architecture.

  ---

  3. Param # (Number of Parameters)

  This column tells you how many trainable weights are in each layer. It's a measure of the layer's complexity and how much memory it requires.

   * Conv2D and Dense layers have parameters to learn.
   * MaxPooling2D and Flatten layers have 0 parameters. They are just simple, fixed operations with nothing to learn.

  You'll notice the Dense layers have a huge number of parameters. This is because every neuron in a dense layer is connected to every neuron in the
  previous layer.

  At the bottom, you see the totals:
   * Total params: The overall complexity of your model.
   * Trainable params: The weights that will be updated during training. In our case, this is all of them.
   * Non-trainable params: Weights that are "frozen." This will be 0 for us now, but will be important when we use pre-trained models later.


    1. Where is the Positive Predictive Value (PPV)?

  You're looking right at it! In the Classification Report, the column named `precision` is exactly the same as Positive Predictive Value (PPV).

   * Precision / PPV: Answers the question, "Of all the times the model predicted a certain class, how often was it correct?"
       * The precision for the "TB" class is your Positive Predictive Value (PPV).
       * The precision for the "Healthy" class is your Negative Predictive Value (NPV). You'll notice the value I printed manually for NPV should match
         the precision score for the "Healthy" class in the report.

  Similarly, the column named `recall` is your Sensitivity.

  2. What is the Confusion Matrix?

  The Confusion Matrix is the source of truth for all these other metrics. It's a simple table that shows you the raw counts of your model's correct
  and incorrect predictions, broken down by class.

  Let's say your confusion matrix looks like this (I'm using example numbers):

   1 Confusion Matrix
   2 [[40  10]
   3  [15  35]]

  Here's how to read it, based on your target_names=['Healthy', 'TB']:

   * The rows represent the Actual / True labels.
   * The columns represent the Predicted labels.


  ┌─────────────────┬────────────────────┬───────────────┐
  │                 │ Predicted: Healthy │ Predicted: TB │
  ├─────────────────┼────────────────────┼───────────────┤
  │ Actual: Healthy │ 40 (TN)            │ 10 (FP)       │
  │ Actual: TB      │ 15 (FN)            │ 35 (TP)       │
  └─────────────────┴────────────────────┴───────────────┘


   * Top-Left (40): True Negatives (TN). The model correctly predicted "Healthy" 40 times.
   * Top-Right (10): False Positives (FP). The model incorrectly predicted "TB" 10 times when the person was actually healthy. (Type I Error)
   * Bottom-Left (15): False Negatives (FN). The model incorrectly predicted "Healthy" 15 times when the person actually had TB. (Type II Error - often
     the most dangerous kind).
   * Bottom-Right (35): True Positives (TP). The model correctly predicted "TB" 35 times.

  This simple grid is incredibly powerful because it shows you exactly what kind of mistakes your model is making.

  3. What is the Classification Report?

  The Classification Report takes the raw numbers from the Confusion Matrix and calculates the useful percentage-based metrics for you.

  Let's look at a sample report:

   1 Classification Report
   2               precision    recall  f1-score   support
   3
   4      Healthy       0.73      0.80      0.76        50
   5           TB       0.78      0.70      0.74        50
   6
   7     accuracy                           0.75       100
   8    macro avg       0.75      0.75      0.75       100
   9 weighted avg       0.75      0.75      0.75       100

   * `precision` (PPV):
       * For the "Healthy" row: When your model predicts "Healthy", it's correct 73% of the time. (This is your NPV).
       * For the "TB" row: When your model predicts "TB", it's correct 78% of the time. (This is your PPV).
   * `recall` (Sensitivity):
       * For the "Healthy" row: Of all the truly healthy people, your model correctly identified 80%. (This is your Specificity).
       * For the "TB" row: Of all the people who truly have TB, your model correctly identified 70%. (This is your Sensitivity).
   * `f1-score`: This is the harmonic mean (a type of average) of precision and recall. It's a great single number for summarizing a model's
     performance, especially if you have a class imbalance (one class has many more samples than the other).
   * `support`: This is simply the number of actual samples in that class (e.g., there were 50 actual "Healthy" images and 50 actual "TB" images).

  By looking at these reports, you can now have a much richer conversation about your model. Instead of just saying "the accuracy is 62%," you can
  say "the model has a sensitivity of X% for detecting TB, but a specificity of Y%, meaning it's more likely to make this kind of error versus that
  kind." This is the language of professional data science.


  ROC - AUC

  An AUC of 0.52 is a very important result. Let's discuss exactly what that means.

  What is AUC and ROC?

  Imagine you have two piles of X-rays on your desk: one pile of confirmed "TB" cases and one pile of confirmed "Healthy" cases.

  You ask your model to assign a score to every single X-ray. A high score means "more likely to be TB," and a low score means "more likely to be
  Healthy."

  Now, you perform a test:
   1. You randomly pick one X-ray from the "TB" pile.
   2. You randomly pick one X-ray from the "Healthy" pile.
   3. You look at the scores the model gave to each.

  The AUC (Area Under the Curve) is the answer to this question:
  > What is the probability that the model gave a higher score to the TB X-ray than to the Healthy X-ray?

   * An AUC of 1.0 would mean the model is perfect. 100% of the time, it will give a higher score to any TB X-ray than to any Healthy X-ray. The two
     piles are perfectly separated by the model's scores.
   * An AUC of 0.5 means the model is no better than flipping a coin. 50% of the time it gets it right, 50% of the time it gets it wrong. The scores
     for the two piles are completely mixed up and inseparable.
   * An AUC of 0.0 would mean the model is perfectly wrong every single time (which is actually useful, because you could just reverse its
     predictions!).

  Your result, AUC = 0.52, means your model's ability to distinguish between a TB case and a Healthy case is only 52%. This is extremely close to the 
  50% random-guess baseline.

  What is the ROC Curve?

  The ROC (Receiver Operating Characteristic) Curve is the visual representation of this separation ability.

   * It plots the True Positive Rate (how many TB cases you find) against the False Positive Rate (how many Healthy cases you wrongly flag as TB) at
     every possible decision threshold.
   * The diagonal dashed line on the plot represents an AUC of 0.5 (random guessing).
   * A good model has a curve that bows up and to the left, far away from that diagonal line.

  Since your AUC is 0.52, your ROC curve will be a line that is very, very close to that diagonal dashed line. It will have almost no "bow" in it.

  What This Result Tells Us

  This is the most honest and sobering metric we've seen so far. While the final accuracy was around 62% (based on a 0.5 threshold), the AUC of 0.52
  tells us that the underlying ability of the model to truly separate the two classes is very weak.

  This strongly confirms our earlier suspicion: our simple model, trained from scratch on a very small dataset, is not powerful enough for this task.
  It has not learned a reliable way to tell the difference between the two classes.

  This result is not a failure; it's a critical finding. It's the data telling us, "This approach is not working. We need a better one."


    Plan for Tomorrow

  Our main goal will be to significantly improve our model's performance by using a more advanced technique.

   1. Create a New Notebook: We'll start a fresh notebook, 03_tuberculosis_detection_transfer_learning.ipynb, to keep our work organized.
   2. Introduce Transfer Learning: I will guide you through loading a state-of-the-art, pre-trained model (like MobileNetV2) that already knows how to
      see the world.
   3. Fine-Tuning: We will "fine-tune" this pre-trained model by replacing its final layer with a new one suited for our specific "TB" vs. "Healthy"
      task and then training only these last few layers.
   4. Evaluation: We will re-run the same comprehensive evaluation (Confusion Matrix, Classification Report, and ROC/AUC curve) on the new model.
   5. Compare Results: We will directly compare the results (especially the AUC) of the new transfer learning model with the baseline model we built
      today to quantify the improvement.

  This process will give you hands-on experience with one of the most powerful and practical techniques in modern computer vision.